{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51010da",
   "metadata": {},
   "source": [
    "<center><h1>Assignment 1</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6e59d",
   "metadata": {},
   "source": [
    "<center><h3>Text generation using N-gram</h2><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489ee68",
   "metadata": {},
   "source": [
    "<center>Saleh Mohammed Al saeed</center>\n",
    "\n",
    "<center>441014299</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd4a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90322087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بسم', 'الله', 'الرحمن', 'الرحيم', 'الحمد', 'لله', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load A corpus into string variable because the data are huge it will take time to predict\n",
    "corpus_path = os.getcwd() + '/Corpus/A'\n",
    "corpus = PlaintextCorpusReader(corpus_path, '.*\\.txt')\n",
    "words = corpus.words()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbe4117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed words: 5,999,258\n",
      "Total of cleaned words: 22,734,267\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "# stop_words = set(stopwords.words('arabic')) \n",
    "# Not needed because result will be not accurate\n",
    "\n",
    "words_cleaned = [word for word in words if not word in ['*', '_', '\"', '.', '،', ',','-','«', '(', ')', '{', '}', '[', ']', ':', '?', '؟'] and not word.isnumeric()]\n",
    "\n",
    "\n",
    "print(f\"Removed words: {(len(words) - len(words_cleaned)):,}\")\n",
    "print(f\"Total of cleaned words: {len(words_cleaned):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b2d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the value of n: 6\n"
     ]
    }
   ],
   "source": [
    "# Ask the user to define value n\n",
    "n = int(input(\"Enter the value of n: \"))\n",
    "\n",
    "while(n > 6 or n < 2):\n",
    "    print(\"n must be between 2 and 6\")\n",
    "    n = int(input(\"Enter the value of n: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c6cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate N-grams\n",
    "ngrams_list = ngrams(words_cleaned, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c66c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency of N-grams\n",
    "ngrams_freq = FreqDist(ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25c14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the frequent sentence with descending order to become faster to find\n",
    "ngrams_freq_sorted = sorted(ngrams_freq.items(), key= lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b81ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram check is for matching the sentence with the trigram and find the best match\n",
    "def trigram_check(start_slice: int, n_gram: int, trigram: tuple, sentence: str):\n",
    "    \n",
    "    context = sentence.split()\n",
    "    if len(context) >= n_gram:\n",
    "        context = context[-n_gram:]\n",
    "        \n",
    "    if list(trigram[-n_gram:]) == context[-n_gram:]:\n",
    "        return [False, random.choice(words_cleaned)]\n",
    "    \n",
    "    if(context[0] in trigram and trigram.index(context[0]) == start_slice and context[0] != trigram[len(trigram) - 1]):\n",
    "        for i in range(start_slice, start_slice + n_gram):\n",
    "            if(i < len(context) and context[i] != trigram[i]):\n",
    "                return [False, random.choice(words_cleaned)]\n",
    "        \n",
    "        try:\n",
    "            return [True, trigram[trigram.index(context[len(context) - 1]) + 1]]\n",
    "        except:\n",
    "            return [False, random.choice(words_cleaned)]\n",
    "    return [False, random.choice(words_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d8fc29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'العالمين'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict next word based on the words in sentence\n",
    "def predict_nextword(sentence: str):\n",
    "    next_word = None\n",
    "    \n",
    "    for trigram, freq in ngrams_freq_sorted:\n",
    "        for i in range(n):\n",
    "            next_word = trigram_check(i, n, trigram, sentence)\n",
    "            if next_word[0]:\n",
    "                return next_word[1]\n",
    "            \n",
    "    return next_word[1]\n",
    "\n",
    "# Try the function\n",
    "predict_nextword(\"الحمد لله رب\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017ebf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate output sentence\n",
    "def generate_sentence(seed_word: str, num_of_words: int):\n",
    "    sentence = \"\" + seed_word.strip() + \" \"\n",
    "    for i in range(num_of_words - len(seed_word.split())):\n",
    "        sentence += predict_nextword(sentence) + \" \"\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6fe6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: وقالوا\n",
      "Enter the number of word want to generate: 6\n"
     ]
    }
   ],
   "source": [
    "# Ask the user to enter first word to generate sentence\n",
    "seed_word = input(\"Enter a word: \")\n",
    "num_of_words = int(input(\"Enter the number of word want to generate: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72859ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'وقالوا لن تمسنا النار إلا أياما '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question1: Test the function\n",
    "generate_sentence(seed_word, num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91644d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question2: the 10 high frequent (trigram) in the corpus\n",
    "\n",
    "# reload the whole corpus\n",
    "corpus_path = os.getcwd() + '/Corpus/'\n",
    "corpus = PlaintextCorpusReader(corpus_path, '.*\\.txt')\n",
    "words = corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f937d018",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Frequent n-gram and the most common n-grams\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ngrams_list \u001b[38;5;241m=\u001b[39m ngrams(words, n)\n\u001b[1;32m----> 3\u001b[0m ngrams_freq \u001b[38;5;241m=\u001b[39m \u001b[43mFreqDist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m most_common_trigram \u001b[38;5;241m=\u001b[39m ngrams_freq\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\probability.py:102\u001b[0m, in \u001b[0;36mFreqDist.__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Construct a new frequency distribution.  If ``samples`` is\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    given, then the frequency distribution will be initialized\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    :type samples: Sequence\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mCounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Cached number of samples in this FreqDist\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:593\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    590\u001b[0m \n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(iterable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\probability.py:140\u001b[0m, in \u001b[0;36mFreqDist.update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03mOverride ``Counter.update()`` to invalidate the cached N\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:679\u001b[0m, in \u001b[0;36mCounter.update\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 679\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:424\u001b[0m, in \u001b[0;36mConcatenatedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_piece \u001b[38;5;241m=\u001b[39m piece\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# Get everything we can from this piece.\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m piece\u001b[38;5;241m.\u001b[39miterate_from(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, start_tok \u001b[38;5;241m-\u001b[39m offset))\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Update the offset table.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m piecenum \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n\u001b[1;32m--> 306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    310\u001b[0m )\n\u001b[0;32m    311\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py:117\u001b[0m, in \u001b[0;36mPlaintextCorpusReader._read_word_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    115\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# Read 20 lines at a time.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     words\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:1100\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.readline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     startpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbytebuffer)\n\u001b[1;32m-> 1100\u001b[0m     new_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;66;03m# If we're at a '\\r', then read one extra character, since\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# it might be a '\\n', to get the proper line ending.\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_chars \u001b[38;5;129;01mand\u001b[39;00m new_chars\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:1333\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader._read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;66;03m# Skip past the byte order marker, if present.\u001b[39;00m\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bom \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bom)\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;66;03m# Read the requested number of bytes.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Frequent n-gram and the most common n-grams\n",
    "ngrams_list = ngrams(words, n)\n",
    "ngrams_freq = FreqDist(ngrams_list)\n",
    "most_common_trigram = ngrams_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the n-grams before removing the symbols\n",
    "for trigram, count in most_common_trigram:\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983632db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the trigram\n",
    "symbols = ['-[', '__________', '*', '/','_', '\"', '.', '،', ',','-','«', '(', ')', '{', '}', '[', ']', ':', '?', '؟']\n",
    "words_cleaned = [word for word in words if not word in symbols and not word.isnumeric()]\n",
    "\n",
    "# n-grams after removing the symbols\n",
    "ngrams_list = ngrams(words_cleaned, n)\n",
    "ngrams_freq = FreqDist(ngrams_list)\n",
    "most_common_trigram = ngrams_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95440751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the n-grams After removing the symbols\n",
    "for trigram, count in most_common_trigram:\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad57b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the most common to a text file\n",
    "with open('most_common_trigram.txt', 'w', encoding='utf-8') as f:\n",
    "    for trigram, freq in most_common_trigram:\n",
    "        f.write(' '.join(trigram) + '\\t' + str(freq) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad06d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
